{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install llama_index\n",
    "!pip install langchain_openai\n",
    "!pip install langchain_community\n",
    "!pip install langgraph\n",
    "!pip install retriever\n",
    "!pip install llama-index-utils-workflow\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import openai\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.core.node_parser import LangchainNodeParser, SentenceWindowNodeParser,SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import BaseExtractor,KeywordExtractor,TitleExtractor\n",
    "from llama_index.core import Settings,SimpleDirectoryReader,StorageContext,VectorStoreIndex,load_index_from_storage, Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings---------- #\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=openai.api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=openai.api_key,\n",
    "    embed_batch_size=100\n",
    ")\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=512,chunk_overlap=50)\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "# Paths for storage\n",
    "DB_DIR = os.getenv(\"DB_DIR\", os.path.join(os.getcwd(), \"docs\", \"chroma\"))\n",
    "INDEX_DIR = os.getenv(\"INDEX_DIR\", os.path.join(os.getcwd(), \"index\"))\n",
    "METADATA_ENRICHMENT_INDEX_DIR = os.getenv(\"METADATA_ENRICHMENT_INDEX_DIR\", os.path.join(os.getcwd(), \"enriched_index\"))\n",
    "\n",
    "# Folder containing the PDF files\n",
    "BASE_DIR = os.getenv(\"BASE_DIR\", os.getcwd())\n",
    "DATA_FOLDER = os.path.join(BASE_DIR, \"docs\")\n",
    "\n",
    "# Settings end----- #\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "\n",
    "def metadata_enrichment_index(files=DATA_FOLDER, documents=None):\n",
    "    \"\"\"Create an enriched index with transformations.\"\"\"\n",
    "    nest_asyncio.apply()\n",
    "    extractors = [\n",
    "        TitleExtractor(nodes=5, llm=Settings.llm),\n",
    "        KeywordExtractor(keywords=10, llm=Settings.llm)\n",
    "    ]\n",
    "    transformations = [Settings.node_parser] + extractors\n",
    "    doc_lis=files\n",
    "    docs_nodes=[] #[node]\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    if documents:\n",
    "        docs_nodes.extend(pipeline.run(documents=docs))\n",
    "    else:\n",
    "        for doc in doc_lis:\n",
    "            docs = SimpleDirectoryReader(input_files=[doc]).load_data()\n",
    "            docs_nodes.extend(pipeline.run(documents=docs))\n",
    "\n",
    "    index = VectorStoreIndex(nodes=docs_nodes,embed_model=Settings.embed_model)\n",
    "    index.storage_context.persist(persist_dir=METADATA_ENRICHMENT_INDEX_DIR)\n",
    "    return index\n",
    "\n",
    "def llama_index_chunk_pdf(files=DATA_FOLDER, index_dir=METADATA_ENRICHMENT_INDEX_DIR):\n",
    "    '''load/create index'''\n",
    "    file_paths = [os.path.join(files, f) for f in os.listdir(files) if f.endswith('.pdf')]\n",
    "    documents = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "    document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "    \n",
    "    if os.path.exists(index_dir):\n",
    "        for file in os.listdir(index_dir):\n",
    "            os.remove(os.path.join(index_dir, file))\n",
    "    else:\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents([document])\n",
    "    index.storage_context.persist(persist_dir=index_dir)\n",
    "    return index\n",
    "\n",
    "def main():\n",
    "    \"\"\"execute chunking process\"\"\"\n",
    "    llama_index_chunk_pdf(files=DATA_FOLDER)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModel, AutoTokenizer\n",
    "from langchain_core.tools import tool\n",
    "from llama_index.core import Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor#, SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.question_gen.prompts import DEFAULT_SUB_QUESTION_PROMPT_TMPL\n",
    "\n",
    "#BAAI method\n",
    "# Define the model and the local directory to save the model\n",
    "#model_name_or_path = \"BAAI/bge-small-en-v1.5\"\n",
    "#local_model_path = \"local_models/bge-small-en-v1.5\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#model = AutoModel.from_pretrained(model_name_or_path)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Save the model and tokenizer to the local path\n",
    "#model.save_pretrained(local_model_path)\n",
    "#tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "#print(f\"Model and tokenizer saved to {local_model_path}\")\n",
    "\n",
    "'''settings'''\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=10,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature= 0.1, timeout=60)\n",
    "#Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\"\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(sentence_index, similarity_top_k=6):\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    #rerank = SentenceTransformerRerank(top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\")\n",
    "    sentence_window_engine = sentence_index.as_query_engine(similarity_top_k=similarity_top_k,\n",
    "                                                            node_postprocessors=[postproc])\n",
    "    return sentence_window_engine\n",
    "\n",
    "def final_engine(engine, verbose=False):\n",
    "    question_gen = LLMQuestionGenerator.from_defaults(\n",
    "        llm=Settings.llm,\n",
    "        prompt_template_str=\"\"\"\n",
    "            Follow the example, but instead of giving a question, always prefix the question\n",
    "            with: 'By first identifying and quoting the most relevant sources, '.\n",
    "            \"\"\"\n",
    "        + DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n",
    "    )\n",
    "    return SubQuestionQueryEngine.from_defaults(\n",
    "        query_engine_tools=[\n",
    "            QueryEngineTool(\n",
    "                query_engine=engine,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"docs\",\n",
    "                    description=\"ESG information and portfolio constructions on companies.\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        question_gen=question_gen,\n",
    "        use_async=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "def llama_index_retriever_tool(index_path: str = METADATA_ENRICHMENT_INDEX_DIR, index_type='sentence', similarity_top_k=6):\n",
    "    \"\"\"\n",
    "    A tool that allows searching and retrieving information from documents using llama-index.\\n\n",
    "    params: Path to the VectorStoreIndex.\\n\n",
    "    return: query engine \\n\n",
    "    usage: llama_index_retriever_tool(path)\n",
    "    \"\"\"\n",
    "    # Load the index \n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    # Create the sentence window query engine from the index\n",
    "    query_engine = get_sentence_window_query_engine(index, similarity_top_k=similarity_top_k) if 'sentence' in index_type else index.as_query_engine(similarity_top_k=similarity_top_k)\n",
    "    query_engine = final_engine(query_engine, verbose=True)\n",
    "\n",
    "    @tool\n",
    "    def engine(query=''):\n",
    "        \"\"\"\n",
    "        A tool that utilizes RAG. It allows searching and retrieving information from documents using llama-index.\\n\n",
    "        params:\n",
    "            query: query to RAG; user query or rephrase the query to find or infer what information needs to be searched.\\n\n",
    "        return: Retrieved information from the vector store of information\\n\n",
    "        usage: engine.invoke(query)\n",
    "        \"\"\"\n",
    "        res=query_engine.query(query)\n",
    "        # display\n",
    "        print(f'---RAG---:\\n {res}')\n",
    "        return res\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MAX_ATTEMPT = 5\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    attempt_num: int\n",
    "\n",
    "\n",
    "### Edges\n",
    "# def grade_documents(state) -> Literal[\"generate\", \"generate_no_ans\"]:\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\", \"generate_no_ans\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    \n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0.1, model=MODEL_NAME, streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"messages:\", messages)\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    print('Question:', question)\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        print(\"docs:\")\n",
    "        print(docs)\n",
    "        return \"generate\"\n",
    "\n",
    "    elif state[\"attempt_num\"] < MAX_ATTEMPT:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        print(\"docs:\")\n",
    "        print(docs)\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT, MAX_ATTEMPT achieved---\")\n",
    "        print(score)\n",
    "        print(\"docs:\")\n",
    "        print(docs)\n",
    "        return \"generate_no_ans\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def agent_with_tools(tools):\n",
    "    def agent(state):\n",
    "        \"\"\"\n",
    "        Invokes the agent model to generate a response based on the current state. Given\n",
    "        the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "        Args:\n",
    "            state (messages): The current state\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated state with the agent response appended to messages\n",
    "        \"\"\"\n",
    "        print(\"---CALL AGENT---\")\n",
    "        messages = state[\"messages\"]\n",
    "        if not state.get(\"attempt_num\"):\n",
    "            state[\"attempt_num\"] = 0  # Initialize attempt number\n",
    "        state[\"attempt_num\"] += 1  # Increment attempt number\n",
    "        model = ChatOpenAI(temperature=0.1, streaming=True, model=MODEL_NAME)\n",
    "        model = model.bind_tools(tools)\n",
    "        response = model.invoke(messages)\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response], \"attempt_num\": state[\"attempt_num\"]}\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0.1, model=MODEL_NAME, streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response], \"attempt_num\": state[\"attempt_num\"]+1}\n",
    "\n",
    "def generate_no_ans(state):\n",
    "    \"\"\"\n",
    "    Generate response when no answer found\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with message stating no relevant info found\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE_NO_ANS---\")\n",
    "    return {\"messages\": [\"No Relevant Info found in the documents\"], \"attempt_num\": 0}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response], \"attempt_num\": 0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gradio as gr\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def build_workflow(vecdb, framework):\n",
    "    retriever_tool = llama_index_retriever_tool(vecdb)\n",
    "    tools = [retriever_tool]\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.attempt_num=0\n",
    "\n",
    "    # Define the nodes we will cycle between\n",
    "    workflow.add_node(\"agent\", agent_with_tools(tools))  # agent\n",
    "    retrieve = ToolNode(tools)\n",
    "    workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "    workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "    workflow.add_node(\"generate_no_ans\", generate_no_ans)  #  Generating a response after we know no document is relevant\n",
    "    workflow.add_node(\n",
    "        \"generate\", generate\n",
    "    )  # Generating a response after we know the documents are relevant\n",
    "    # Call agent node to decide to retrieve or not\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "\n",
    "    # Decide whether to retrieve\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        # Assess agent decision\n",
    "        tools_condition,\n",
    "        {\n",
    "            # Translate the condition outputs to nodes in our graph\n",
    "            \"tools\": \"retrieve\",\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Edges taken after the `action` node is called.\n",
    "    workflow.add_conditional_edges(\n",
    "        \"retrieve\",\n",
    "        # Assess agent decision\n",
    "        grade_documents,\n",
    "    )\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    workflow.add_edge(\"generate_no_ans\", END)\n",
    "    workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Gradio Integration\n",
    "def get_answer_func(graph):\n",
    "    def get_answer_chat(question, history):\n",
    "        print(\"User question:\", question)\n",
    "        result = graph.invoke(\n",
    "            {\"messages\": [HumanMessage(content=question)]},\n",
    "            config={\"configurable\": {\"thread_id\": 42, \"max_attempt\": 5}}\n",
    "        )\n",
    "        return result[\"messages\"][-1].content\n",
    "    return get_answer_chat\n",
    "\n",
    "index_path = os.path.join(os.getcwd(), \"enriched_index\")\n",
    "graph = build_workflow(index_path, sys.argv[1])\n",
    "\n",
    "gr.ChatInterface(\n",
    "    get_answer_func(graph),\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    title=\"Agent\",\n",
    "    description=\"Ask me any question\",\n",
    "    theme=\"soft\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be improved:\n",
    "\n",
    "1. Runtime\n",
    "2. Attempt Time\n",
    "3. Data loading process"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
